import { BaseMessage, AIMessage, HumanMessage } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";
import { TokenUsage, CompressionRecord } from "../types";

// 8æ®µå¼å‹ç¼©æç¤ºè¯
const COMPRESSION_PROMPT = `Your task is to create a detailed summary of the conversation so far, 
paying close attention to the user's explicit requests and your previous actions.
This summary should be thorough in capturing technical details, code patterns, 
and architectural decisions that would be essential for continuing development 
work without losing context.

Before providing your final summary, wrap your analysis in <analysis> tags to 
organize your thoughts and ensure you've covered all necessary points.

Your summary should include the following sections:

1. **Primary Request and Intent**: Capture all of the user's explicit requests and intents in detail
2. **Key Technical Concepts**: List all important technical concepts, technologies, and frameworks discussed.
3. **Files and Code Sections**: Enumerate specific files and code sections examined, modified, or created. Pay special attention to the most recent messages and include full code snippets where applicable.
4. **Errors and fixes**: List all errors that you ran into, and how you fixed them. Pay special attention to specific user feedback.
5. **Problem Solving**: Document problems solved and any ongoing troubleshooting efforts.
6. **All user messages**: List ALL user messages that are not tool results. These are critical for understanding the users' feedback and changing intent.
7. **Pending Tasks**: Outline any pending tasks that you have explicitly been asked to work on.
8. **Current Work**: Describe in detail precisely what was being worked on immediately before this summary request.
9. **Optional Next Step**: List the next step that you will take that is related to the most recent work you were doing.`;

// Token ä½¿ç”¨ç»Ÿè®¡ç±»
export class TokenCounter {
  private maxTokens: number;
  private compressionThreshold: number;

  constructor(maxTokens: number = 128000, compressionThreshold: number = 0.92) {
    this.maxTokens = maxTokens;
    this.compressionThreshold = compressionThreshold;
  }

  // è·å–æœ€æ–°çš„ Token ä½¿ç”¨æƒ…å†µï¼ˆå€’åºæŸ¥æ‰¾ï¼‰
  getLatestTokenUsage(messages: BaseMessage[]): TokenUsage {
    console.log('ğŸ” æ£€æŸ¥è®°å¿†ä½¿ç”¨æƒ…å†µ...');
    
    let totalTokens = 0;
    // èªæ˜çš„åœ°æ–¹ï¼šä»æœ€æ–°æ¶ˆæ¯å¼€å§‹æ‰¾ï¼Œå› ä¸º usage ä¿¡æ¯é€šå¸¸åœ¨æœ€è¿‘çš„æ¶ˆæ¯é‡Œ
    for (let i = messages.length - 1; i >= 0; i--) {
      const message = messages[i];
      if (message instanceof AIMessage && message.response_metadata?.usage) {
        const usage = message.response_metadata.usage;
        totalTokens = this.calculateTotalTokens(usage);
        break; // æ‰¾åˆ°å°±åœæ­¢ï¼Œä¸æµªè´¹æ—¶é—´
      }
    }

    // å¦‚æœæ²¡æœ‰æ‰¾åˆ° usage ä¿¡æ¯ï¼Œä¼°ç®— token æ•°é‡
    if (totalTokens === 0) {
      totalTokens = this.estimateTokens(messages);
    }

    return {
      used: totalTokens,
      total: this.maxTokens,
      percentage: totalTokens / this.maxTokens,
    };
  }

  // è®¡ç®—æ€» Token æ•°ï¼ˆåŒ…å«ç¼“å­˜ï¼‰
  private calculateTotalTokens(usage: any): number {
    return (usage.total_tokens || 0) + (usage.cache_creation_tokens || 0);
  }

  // ä¼°ç®— Token æ•°é‡ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
  private estimateTokens(messages: BaseMessage[]): number {
    const totalContent = messages.map(m => m.content.toString()).join(' ');
    // ç®€å•çš„ä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
    return Math.ceil(totalContent.length / 4);
  }

  // æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
  needsCompression(messages: BaseMessage[]): boolean {
    const usage = this.getLatestTokenUsage(messages);
    return usage.percentage >= this.compressionThreshold;
  }
}

// ä¸Šä¸‹æ–‡å‹ç¼©å™¨ç±»
export class ContextCompressor {
  private model: ChatOpenAI;
  private tokenCounter: TokenCounter;

  constructor(model: ChatOpenAI, maxTokens?: number, compressionThreshold?: number) {
    this.model = model;
    this.tokenCounter = new TokenCounter(maxTokens, compressionThreshold);
  }

  // æ‰§è¡Œ8æ®µå¼å‹ç¼©
  async compress(messages: BaseMessage[]): Promise<{
    compressedMessage: AIMessage;
    compressionRecord: CompressionRecord;
    removedMessages: BaseMessage[];
  }> {
    const beforeUsage = this.tokenCounter.getLatestTokenUsage(messages);
    
    console.log(`ğŸ—œï¸ å¼€å§‹å‹ç¼©ï¼Œå½“å‰ä½¿ç”¨: ${beforeUsage.used} tokens (${(beforeUsage.percentage * 100).toFixed(1)}%)`);

    // ç”Ÿæˆå‹ç¼©æ¶ˆæ¯çš„å†…å®¹
    const messagesContent = this.formatMessagesForCompression(messages);
    const compressionInput = `${COMPRESSION_PROMPT}\n\nConversation to summarize:\n${messagesContent}`;

    // è°ƒç”¨æ¨¡å‹è¿›è¡Œå‹ç¼©
    const compressedSummary = await this.model.invoke([
      new HumanMessage(compressionInput)
    ]);

    // åˆ›å»ºå‹ç¼©åçš„æ¶ˆæ¯
    const compressedMessage = new AIMessage(
      `[COMPRESSED SUMMARY]\n\n${compressedSummary.content}`
    );

    // è®¡ç®—å‹ç¼©æ•ˆæœ
    const afterTokens = this.tokenCounter.estimateTokens([compressedMessage]);
    const compressionRatio = afterTokens / beforeUsage.used;

    const compressionRecord: CompressionRecord = {
      timestamp: new Date().toISOString(),
      beforeTokens: beforeUsage.used,
      afterTokens,
      compressionRatio,
      summary: compressedSummary.content.toString().substring(0, 200) + '...'
    };

    console.log(`âœ… å‹ç¼©å®Œæˆ: ${beforeUsage.used} -> ${afterTokens} tokens (${(compressionRatio * 100).toFixed(1)}%)`);

    // å†³å®šç§»é™¤å“ªäº›æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘çš„ä¸€äº›æ¶ˆæ¯ï¼‰
    const recentMessagesToKeep = 5;
    const removedMessages = messages.slice(0, -recentMessagesToKeep);

    return {
      compressedMessage,
      compressionRecord,
      removedMessages
    };
  }

  // æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºå‹ç¼©
  private formatMessagesForCompression(messages: BaseMessage[]): string {
    return messages.map((msg, index) => {
      const role = msg instanceof HumanMessage ? 'Human' : 
                   msg instanceof AIMessage ? 'Assistant' : 
                   'System';
      const content = msg.content.toString().substring(0, 1000); // é™åˆ¶é•¿åº¦
      return `[${index + 1}] ${role}: ${content}`;
    }).join('\n\n');
  }

  // æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
  needsCompression(messages: BaseMessage[]): boolean {
    return this.tokenCounter.needsCompression(messages);
  }

  // è·å–å½“å‰ä½¿ç”¨æƒ…å†µ
  getCurrentUsage(messages: BaseMessage[]): TokenUsage {
    return this.tokenCounter.getLatestTokenUsage(messages);
  }
}
